#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BBåå½ˆMLæ¨¡å‹ - å¾Hugging Faceå®Œæ•´è¨“ç·´æµç¨‹
ä¸€å€‹æ–‡ä»¶åŒ…å«æ‰€æœ‰æ­¥é©Ÿï¼Œç›´æ¥é‹è¡Œå³å¯
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from huggingface_hub import hf_hub_download
import ta
import warnings
import pickle
import os
import json
from pathlib import Path

warnings.filterwarnings('ignore')

# ============================================================================
# é…ç½®
# ============================================================================

class Config:
    # æ•¸æ“šé…ç½®
    SYMBOLS = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']  # ä¿®æ”¹æˆæ‚¨è¦çš„å¹£ç¨®
    TIMEFRAME = '15m'  # 15m æˆ– 1h
    
    # HF é…ç½®
    HF_REPO_ID = "zongowo111/v2-crypto-ohlcv-data"
    
    # è¨“ç·´é…ç½®
    LOOK_AHEAD = 5  # è§¸åŠå¾Œå¾€å‰çœ‹å¹¾æ ¹Kæ£’
    SUCCESS_THRESHOLD = 0.5  # æˆåŠŸåå½ˆçš„æœ€å°ç™¾åˆ†æ¯”
    
    # è·¯å¾‘é…ç½®
    DATA_CACHE_DIR = './data_cache'
    PROCESSED_DATA_DIR = './processed_data'
    LABELS_DIR = './labels'
    FEATURES_DIR = './features'
    MODELS_DIR = './models'
    
    # æ¨¡å‹é…ç½®
    TEST_SIZE = 0.2
    RANDOM_STATE = 42
    
    @staticmethod
    def create_dirs():
        """å‰µå»ºæ‰€éœ€çš„ç›®éŒ„"""
        for dir_path in [Config.DATA_CACHE_DIR, Config.PROCESSED_DATA_DIR, 
                         Config.LABELS_DIR, Config.FEATURES_DIR, Config.MODELS_DIR]:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
        print("âœ… ç›®éŒ„å‰µå»ºå®Œæˆ")

# ============================================================================
# ç¬¬ä¸€æ­¥ï¼šæ•¸æ“šè®€å–
# ============================================================================

def download_from_hf(symbol, timeframe):
    """å¾Hugging Faceä¸‹è¼‰æ•¸æ“š"""
    print(f"\nğŸ“¥ æ­£åœ¨ä¸‹è¼‰ {symbol} {timeframe} æ•¸æ“š...")
    
    file_path = f"klines/{symbol}/{symbol.split('USDT')[0]}_{timeframe}.parquet"
    
    try:
        local_path = hf_hub_download(
            repo_id=Config.HF_REPO_ID,
            filename=file_path,
            repo_type="dataset",
            cache_dir=Config.DATA_CACHE_DIR
        )
        
        df = pd.read_parquet(local_path)
        
        # è™•ç†ç´¢å¼• - é—œéµä¿®å¾©ï¼æ”¯æŒå¤šç¨®æ™‚é–“æˆ³æ ¼å¼
        if not isinstance(df.index, pd.DatetimeIndex):
            if 'timestamp' in df.columns:
                try:
                    # å…ˆå˜—è©¦æ¯«ç§’æ™‚é–“æˆ³
                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                except:
                    try:
                        # å†å˜—è©¦ç§’æ™‚é–“æˆ³
                        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
                    except:
                        # æœ€å¾Œå˜—è©¦å­—ç¬¦ä¸²æ ¼å¼
                        df['timestamp'] = pd.to_datetime(df['timestamp'])
                df.set_index('timestamp', inplace=True)
            elif 'datetime' in df.columns:
                try:
                    df['datetime'] = pd.to_datetime(df['datetime'])
                except:
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼
                    df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d %H:%M:%S')
                df.set_index('datetime', inplace=True)
        
        # çµ±ä¸€åˆ—å
        df.columns = df.columns.str.lower()
        
        # ç¢ºä¿æœ‰å¿…éœ€çš„åˆ—
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        if not all(col in df.columns for col in required_cols):
            print(f"   âš ï¸  ç¼ºå°‘å¿…éœ€åˆ—ï¼Œå˜—è©¦åˆ—åæ˜ å°„...")
            # å˜—è©¦å¸¸è¦‹çš„åˆ—åæ˜ å°„
            rename_map = {
                'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume',
                'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'
            }
            df.rename(columns=rename_map, inplace=True)
        
        print(f"   âœ… ä¸‹è¼‰æˆåŠŸ: {len(df)} è¡Œ")
        print(f"   æ™‚é–“ç¯„åœ: {df.index[0]} ~ {df.index[-1]}")
        print(f"   åˆ—: {list(df.columns[:10])}...")
        
        return df
    
    except Exception as e:
        print(f"   âŒ ä¸‹è¼‰å¤±æ•—: {str(e)}")
        import traceback
        print(f"   è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        return None

def download_all_data():
    """æ‰¹é‡ä¸‹è¼‰æ‰€æœ‰å¹£ç¨®æ•¸æ“š"""
    print(f"\n{'='*60}")
    print(f"æ­¥é©Ÿ1ï¼šå¾HFä¸‹è¼‰æ•¸æ“š")
    print(f"{'='*60}")
    
    all_data = {}
    
    for symbol in Config.SYMBOLS:
        df = download_from_hf(symbol, Config.TIMEFRAME)
        if df is not None:
            all_data[symbol] = df
    
    return all_data

# ============================================================================
# ç¬¬äºŒæ­¥ï¼šè¨ˆç®—æŠ€è¡“æŒ‡æ¨™
# ============================================================================

def add_technical_indicators(df):
    """æ·»åŠ æ‰€æœ‰æŠ€è¡“æŒ‡æ¨™"""
    print(f"   è¨ˆç®—æŒ‡æ¨™ä¸­...", end='', flush=True)
    
    df = df.copy()
    
    # Bollinger Bands - ä¿®å¾©APIèª¿ç”¨
    bb_indicator = ta.volatility.BollingerBands(df['close'], window=20, window_dev=2)
    df['bb_upper'] = bb_indicator.bollinger_hband()
    df['bb_middle'] = bb_indicator.bollinger_mavg()
    df['bb_lower'] = bb_indicator.bollinger_lband()
    df['bb_width'] = df['bb_upper'] - df['bb_lower']
    df['bb_width_ma'] = df['bb_width'].rolling(20).mean()
    df['bb_width_ratio'] = df['bb_width'] / (df['bb_width_ma'] + 0.0001)
    
    # RSI
    df['rsi'] = ta.momentum.RSIIndicator(df['close'], window=14).rsi()
    
    # MACD
    macd_indicator = ta.trend.MACD(df['close'])
    df['macd'] = macd_indicator.macd()
    df['macd_signal'] = macd_indicator.macd_signal()
    df['macd_hist'] = macd_indicator.macd_diff()
    
    # ATR
    atr_indicator = ta.volatility.AverageTrueRange(df['high'], df['low'], df['close'], window=14)
    df['atr'] = atr_indicator.average_true_range()
    df['atr_ratio'] = df['atr'] / df['close']
    
    # æˆäº¤é‡
    df['vol_ma20'] = df['volume'].rolling(20).mean()
    df['vol_ratio'] = df['volume'] / (df['vol_ma20'] + 0.0001)
    
    # å‹•èƒ½
    df['roc'] = df['close'].pct_change(5)
    df['momentum'] = df['close'] - df['close'].shift(5)
    
    # ç§»å‹•å¹³å‡ç·š
    df['ema9'] = ta.trend.EMAIndicator(df['close'], window=9).ema_indicator()
    df['ema21'] = ta.trend.EMAIndicator(df['close'], window=21).ema_indicator()
    df['sma20'] = df['close'].rolling(20).mean()
    df['sma200'] = df['close'].rolling(200).mean()
    
    # Kç·šå½¢æ…‹
    df['body_size'] = abs(df['close'] - df['open'])
    df['body_ratio'] = df['body_size'] / (df['high'] - df['low'] + 0.0001)
    df['upper_wick'] = df['high'] - df[['close', 'open']].max(axis=1)
    df['lower_wick'] = df[['close', 'open']].min(axis=1) - df['low']
    df['wick_ratio'] = (df['upper_wick'] + df['lower_wick']) / (df['high'] - df['low'] + 0.0001)
    df['high_low_range'] = df['high'] - df['low']
    
    # ADX
    adx_indicator = ta.trend.ADXIndicator(df['high'], df['low'], df['close'], window=14)
    df['adx'] = adx_indicator.adx()
    
    # è™•ç† NaN
    df = df.fillna(method='bfill').fillna(method='ffill')
    
    print(f" âœ… {len(df.columns)} åˆ—")
    
    return df

def process_all_data(all_data):
    """è™•ç†æ‰€æœ‰æ•¸æ“šï¼Œæ·»åŠ æŒ‡æ¨™"""
    print(f"\n{'='*60}")
    print(f"æ­¥é©Ÿ2ï¼šè¨ˆç®—æŠ€è¡“æŒ‡æ¨™")
    print(f"{'='*60}\n")
    
    data_with_indicators = {}
    
    for symbol, df in all_data.items():
        print(f"{symbol:10s}", end='')
        processed_df = add_technical_indicators(df)
        data_with_indicators[symbol] = processed_df
    
    return data_with_indicators

# ============================================================================
# ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ¨™ç±¤
# ============================================================================

def create_bounce_labels(df, symbol):
    """ç‚ºBBè§¸åŠäº‹ä»¶ç”Ÿæˆæ¨™ç±¤"""
    labels_list = []
    
    for i in range(len(df) - Config.LOOK_AHEAD - 1):
        current_row = df.iloc[i]
        close_price = current_row['close']
        bb_upper = current_row['bb_upper']
        bb_lower = current_row['bb_lower']
        
        # ä¸‹è»Œè§¸åŠ
        if close_price <= bb_lower * 1.005:
            future_prices = df.iloc[i:i+Config.LOOK_AHEAD+1]['high']
            max_price = future_prices.max()
            price_increase_pct = ((max_price - close_price) / close_price) * 100
            is_success = 1 if price_increase_pct > Config.SUCCESS_THRESHOLD else 0
            
            labels_list.append({
                'symbol': symbol,
                'index': i,
                'timestamp': df.index[i],
                'bounce_type': 'lower',
                'touch_price': close_price,
                'label': is_success,
                'success_pct': price_increase_pct
            })
        
        # ä¸Šè»Œè§¸åŠ
        if close_price >= bb_upper * 0.995:
            future_prices = df.iloc[i:i+Config.LOOK_AHEAD+1]['low']
            min_price = future_prices.min()
            price_decrease_pct = ((close_price - min_price) / close_price) * 100
            is_success = 1 if price_decrease_pct > Config.SUCCESS_THRESHOLD else 0
            
            labels_list.append({
                'symbol': symbol,
                'index': i,
                'timestamp': df.index[i],
                'bounce_type': 'upper',
                'touch_price': close_price,
                'label': is_success,
                'success_pct': price_decrease_pct
            })
    
    return pd.DataFrame(labels_list)

def generate_all_labels(data_with_indicators):
    """ç‚ºæ‰€æœ‰å¹£ç¨®ç”Ÿæˆæ¨™ç±¤"""
    print(f"\n{'='*60}")
    print(f"æ­¥é©Ÿ3ï¼šç”Ÿæˆæ¨™ç±¤")
    print(f"{'='*60}\n")
    
    all_labels = {}
    
    for symbol, df in data_with_indicators.items():
        labels = create_bounce_labels(df, symbol)
        all_labels[symbol] = labels
        success_rate = labels['label'].mean() if len(labels) > 0 else 0
        print(f"{symbol:10s} {len(labels):5d} å€‹è§¸åŠäº‹ä»¶ï¼ŒæˆåŠŸç‡ {success_rate:.2%}")
    
    return all_labels

# ============================================================================
# ç¬¬å››æ­¥ï¼šç‰¹å¾µæå–
# ============================================================================

def extract_bounce_features(df, labels_df):
    """æå–åå½ˆç‰¹å¾µ"""
    features_list = []
    
    for _, label_row in labels_df.iterrows():
        idx = label_row['index']
        
        if idx < 50:
            continue
        
        current_row = df.iloc[idx]
        bounce_type = label_row['bounce_type']
        
        # Kç·šå½¢æ…‹
        body_ratio = current_row['body_ratio']
        wick_ratio = current_row['wick_ratio']
        high_low_range = current_row['high_low_range']
        
        # æˆäº¤é‡
        vol_ratio = current_row['vol_ratio']
        vol_spike_ratio = current_row['volume'] / (current_row['vol_ma20'] + 0.0001)
        
        # å‹•èƒ½
        rsi = current_row['rsi']
        macd = current_row['macd']
        macd_hist = current_row['macd_hist']
        momentum = current_row['momentum']
        
        # BB
        bb_width_ratio = current_row['bb_width_ratio']
        bb_position = (current_row['close'] - current_row['bb_lower']) / (current_row['bb_upper'] - current_row['bb_lower'] + 0.0001)
        
        # è¶¨å‹¢
        recent_close = df.iloc[max(0, idx-20):idx]['close']
        price_trend = 1 if len(recent_close) >= 2 and recent_close.iloc[-1] > recent_close.iloc[0] else 0
        price_slope = (recent_close.iloc[-1] - recent_close.iloc[0]) / recent_close.iloc[0] if len(recent_close) >= 2 else 0
        
        # æ™‚é–“
        timestamp = df.index[idx]
        hour = timestamp.hour
        is_high_volume_time = 1 if (hour >= 20 or hour < 4) else 0
        
        # ADX
        adx = current_row['adx']
        
        feature_dict = {
            'body_ratio': body_ratio,
            'wick_ratio': wick_ratio,
            'high_low_range': high_low_range,
            'vol_ratio': vol_ratio,
            'vol_spike_ratio': vol_spike_ratio,
            'rsi': rsi,
            'macd': macd,
            'macd_hist': macd_hist,
            'momentum': momentum,
            'bb_width_ratio': bb_width_ratio,
            'bb_position': bb_position,
            'price_trend': price_trend,
            'price_slope': price_slope,
            'hour': hour,
            'is_high_volume_time': is_high_volume_time,
            'adx': adx,
            'label': label_row['label'],
            'bounce_type': bounce_type
        }
        
        features_list.append(feature_dict)
    
    return pd.DataFrame(features_list)

def extract_all_features(data_with_indicators, all_labels):
    """ç‚ºæ‰€æœ‰å¹£ç¨®æå–ç‰¹å¾µ"""
    print(f"\n{'='*60}")
    print(f"æ­¥é©Ÿ4ï¼šæå–ç‰¹å¾µ")
    print(f"{'='*60}\n")
    
    all_features = {}
    
    for symbol in data_with_indicators.keys():
        df = data_with_indicators[symbol]
        labels = all_labels[symbol]
        features = extract_bounce_features(df, labels)
        all_features[symbol] = features
        
        success_rate = features['label'].mean() if len(features) > 0 else 0
        print(f"{symbol:10s} {len(features):5d} å€‹æ¨£æœ¬ï¼ŒæˆåŠŸç‡ {success_rate:.2%}")
    
    return all_features

# ============================================================================
# ç¬¬äº”æ­¥ï¼šæ¨¡å‹è¨“ç·´
# ============================================================================

def prepare_training_data(all_features):
    """æº–å‚™è¨“ç·´æ•¸æ“š"""
    print(f"\n{'='*60}")
    print(f"æ­¥é©Ÿ5ï¼šæº–å‚™è¨“ç·´æ•¸æ“š")
    print(f"{'='*60}\n")
    
    # åˆä½µæ‰€æœ‰ç‰¹å¾µ
    combined_features = pd.concat(all_features.values(), ignore_index=True)
    
    # ç‰¹å¾µåˆ—
    feature_cols = [col for col in combined_features.columns 
                    if col not in ['label', 'bounce_type']]
    
    X = combined_features[feature_cols]
    y = combined_features['label']
    
    # è™•ç†ç¼ºå¤±å€¼
    X = X.fillna(X.mean())
    
    # æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=Config.TEST_SIZE, 
        random_state=Config.RANDOM_STATE, stratify=y
    )
    
    print(f"è¨“ç·´é›†å¤§å°: {X_train.shape[0]}")
    print(f"æ¸¬è©¦é›†å¤§å°: {X_test.shape[0]}")
    print(f"ç‰¹å¾µæ•¸: {len(feature_cols)}")
    print(f"æ­£æ¨£æœ¬: {(y==1).sum()} ({y.mean():.2%})")
    print(f"è² æ¨£æœ¬: {(y==0).sum()} ({(1-y.mean()):.2%})")
    
    # ä¿å­˜ scaler
    with open(f'{Config.MODELS_DIR}/scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    # ä¿å­˜ç‰¹å¾µåˆ—å
    with open(f'{Config.MODELS_DIR}/feature_cols.json', 'w') as f:
        json.dump(feature_cols, f)
    
    return X_train, X_test, y_train, y_test, feature_cols

def train_models(X_train, X_test, y_train, y_test, feature_cols):
    """è¨“ç·´å¤šå€‹æ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"æ­¥é©Ÿ6ï¼šè¨“ç·´æ¨¡å‹")
    print(f"{'='*60}\n")
    
    models = {}
    results = {}
    
    # Random Forest
    print("è¨“ç·´ Random Forest...", end='', flush=True)
    rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train)
    models['RandomForest'] = rf
    
    rf_pred = rf.predict(X_test)
    rf_proba = rf.predict_proba(X_test)[:, 1]
    results['RandomForest'] = {
        'accuracy': accuracy_score(y_test, rf_pred),
        'precision': precision_score(y_test, rf_pred, zero_division=0),
        'recall': recall_score(y_test, rf_pred, zero_division=0),
        'f1': f1_score(y_test, rf_pred, zero_division=0),
        'auc': roc_auc_score(y_test, rf_proba)
    }
    print(f" âœ… AUC={results['RandomForest']['auc']:.4f}")
    
    # XGBoost
    print("è¨“ç·´ XGBoost...", end='', flush=True)
    xgb = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.05, random_state=42)
    xgb.fit(X_train, y_train)
    models['XGBoost'] = xgb
    
    xgb_pred = xgb.predict(X_test)
    xgb_proba = xgb.predict_proba(X_test)[:, 1]
    results['XGBoost'] = {
        'accuracy': accuracy_score(y_test, xgb_pred),
        'precision': precision_score(y_test, xgb_pred, zero_division=0),
        'recall': recall_score(y_test, xgb_pred, zero_division=0),
        'f1': f1_score(y_test, xgb_pred, zero_division=0),
        'auc': roc_auc_score(y_test, xgb_proba)
    }
    print(f" âœ… AUC={results['XGBoost']['auc']:.4f}")
    
    # Gradient Boosting
    print("è¨“ç·´ Gradient Boosting...", end='', flush=True)
    gb = GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)
    gb.fit(X_train, y_train)
    models['GradientBoosting'] = gb
    
    gb_pred = gb.predict(X_test)
    gb_proba = gb.predict_proba(X_test)[:, 1]
    results['GradientBoosting'] = {
        'accuracy': accuracy_score(y_test, gb_pred),
        'precision': precision_score(y_test, gb_pred, zero_division=0),
        'recall': recall_score(y_test, gb_pred, zero_division=0),
        'f1': f1_score(y_test, gb_pred, zero_division=0),
        'auc': roc_auc_score(y_test, gb_proba)
    }
    print(f" âœ… AUC={results['GradientBoosting']['auc']:.4f}")
    
    # é¸æ“‡æœ€ä½³æ¨¡å‹
    best_model_name = max(results.keys(), key=lambda x: results[x]['auc'])
    best_model = models[best_model_name]
    
    print(f"\næœ€ä½³æ¨¡å‹: {best_model_name} (AUC={results[best_model_name]['auc']:.4f})")
    
    # ä¿å­˜æ¨¡å‹
    with open(f'{Config.MODELS_DIR}/best_model.pkl', 'wb') as f:
        pickle.dump(best_model, f)
    
    # é¡¯ç¤ºè©³ç´°çµæœ
    print(f"\n{'='*60}")
    print(f"æ¨¡å‹è©•ä¼°çµæœ")
    print(f"{'='*60}")
    for model_name, metrics in results.items():
        print(f"\n{model_name}:")
        print(f"  æº–ç¢ºç‡: {metrics['accuracy']:.4f}")
        print(f"  ç²¾ç¢ºç‡: {metrics['precision']:.4f}")
        print(f"  å¬å›ç‡: {metrics['recall']:.4f}")
        print(f"  F1åˆ†æ•¸: {metrics['f1']:.4f}")
        print(f"  AUC: {metrics['auc']:.4f}")
    
    # ç‰¹å¾µé‡è¦æ€§
    print(f"\n{'='*60}")
    print(f"ç‰¹å¾µé‡è¦æ€§ (Top 10)")
    print(f"{'='*60}")
    
    if hasattr(best_model, 'feature_importances_'):
        importances = best_model.feature_importances_
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        print()
        for i, row in feature_importance.head(10).iterrows():
            print(f"  {row['feature']:20s}: {row['importance']:.4f}")
    
    return best_model, results

# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    print(f"\n{'='*60}")
    print(f"BBåå½ˆMLæ¨¡å‹è¨“ç·´ - å®Œæ•´æµç¨‹")
    print(f"{'='*60}")
    
    # å‰µå»ºç›®éŒ„
    Config.create_dirs()
    
    # æ­¥é©Ÿ1ï¼šä¸‹è¼‰æ•¸æ“š
    all_data = download_all_data()
    if not all_data:
        print("âŒ ç„¡æ³•ä¸‹è¼‰æ•¸æ“šï¼Œé€€å‡º")
        return
    
    # æ­¥é©Ÿ2ï¼šè¨ˆç®—æŒ‡æ¨™
    data_with_indicators = process_all_data(all_data)
    
    # æ­¥é©Ÿ3ï¼šç”Ÿæˆæ¨™ç±¤
    all_labels = generate_all_labels(data_with_indicators)
    
    # æ­¥é©Ÿ4ï¼šæå–ç‰¹å¾µ
    all_features = extract_all_features(data_with_indicators, all_labels)
    
    # æ­¥é©Ÿ5ï¼šæº–å‚™æ•¸æ“š
    X_train, X_test, y_train, y_test, feature_cols = prepare_training_data(all_features)
    
    # æ­¥é©Ÿ6ï¼šè¨“ç·´æ¨¡å‹
    best_model, results = train_models(X_train, X_test, y_train, y_test, feature_cols)
    
    print(f"\n{'='*60}")
    print(f"âœ… è¨“ç·´å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {Config.MODELS_DIR}/best_model.pkl")
    print(f"æº–å‚™é€²è¡Œéƒ¨ç½²æˆ–æ¸¬è©¦...\n")

if __name__ == '__main__':
    main()
